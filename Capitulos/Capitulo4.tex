% Capíulo 4
\chapter{Avaliação} \label{ch:avaliacao}

Este capítulo descreve o estudo empírico realizado em dois sistemas \textit{open-source} de diferentes domínios cujo objetivo é avaliar se as visualizações são úteis para representar os desvios de desempenho encontrados durante a análise desses sistemas, além de avaliar a facilidade de se encontrar informações e a possível aplicabilidade da ferramenta como parte integrande dos processos de desenvolvimento dos sistemas analisados.

O restante deste capítulo está organizado como segue: seção \ref{sec:avaliacao-projeto} apresenta como o estudo foi projetado, incluindo as principais contribuições (subseção \ref{subsec:avaliacao-principais-contribuicoes}), os objetivos e questões de pesquisa (subseção \ref{subsec:avaliacao-objetivos-questoes-pesquisa}), os sistemas analisados e os procedimentos do estudo (subseção \ref{subsec:avaliacao-sistemas-procedimentos}); a seção \ref{sec:avaliacao-resultados} exibe os resultados do estudo e a seção \ref{sec:avaliacao-consideracoes} conclui o capítulo, apresentando as ameaças à validade e discussões sobre os resultados.

\section{Projeto} \label{sec:avaliacao-projeto}

O estudo analisou um total de 20 \textit{releases} dos projetos Jetty \cite{Jetty2016} e VRaptor \cite{VRaptor2017}, sendo 10 \textit{releases} para cada sistema, considerando 193 cenários distintos ao total, gerando as visualizações de sumarização dos cenários e grafo de chamadas para todos os casos. Sobre as visualizações, foram coletados \textit{feedback} de usuários desses sistemas através de um questionário online.

De maneira geral, foram aplicadas as fases discutidas no capítulo \ref{ch:pqae}, subseção \ref{subsec:funcionamento-perfminer}, além do procedimento necessário para gerar os dados para as visualizações, conforme descrito na seção \ref{sec:visao-geral-architecture-qa-evolution} do mesmo capítulo. Em suma: (i) primeiro, os casos de testes automatizados (cada um deles representando um cenário) dos sistemas foram executados e monitorados, resultando em múltiplos bancos de dados; (ii) depois, os dados coletados foram comparados, agrupando \textit{releases} subsequentes para cada sistema; (iii) então, os elementos identificados com desvios de desempenho foram minerados nos seus sistemas de controle de versão, com o intuito de descobrir os \textit{commits} que alteraram esses artefatos; (iv) por fim, foi executado, para cada versão de cada sistema, o procedimento necessário para gerar os dados que dão suporte às visualizações. Após isso, o questionário online foi aplicado.

O questionário foi enviado para todos os contribuidores dos sistemas que possuiam endereço de email registrado na página de contribuidores de cada sistema ou na página do seu perfil. Dos contribuidores que receberam o questionário, X deles responderam com algum \textit{feedback}.

\subsection{Principais Contribuições} \label{subsec:avaliacao-principais-contribuicoes}

As principais contribuições desta avaliação são: (i) a identificação visual dos cenários que mais tiveram desvios de desempenho para os sistemas Jetty e VRaptor, a partir da análise de múltiplas \textit{releases}; (ii) para os mesmos sistemas, a percepção visual do tipo de desvio dos cenários através da análise de múltiplas \textit{releases}; (iii) a identificação visual das potenciais causas dos desvios de desempenho dos cenários dos sistemas analisados; (iv) ...

{\color{red}CONTINUAR...}

\subsection{Objetivos e Questões de Pesquisa} \label{subsec:avaliacao-objetivos-questoes-pesquisa}

Os principais objetivos deste estudo é investigar se a ferramenta \textit{\toolName}, com suas visualizações e propriedades visuais oferecidas, é capaz de representar as evoluções de desempenho dos cenários analisados, se proporciona uma fácil identificação desses desvios pelos usuários e se é útil para as equipes de desenvolvimento dos sistemas analisados. Ao utilizar a ferramenta, os usuários poderão identificar os cenários com desvios de desempenho e, a partir da análise do grafo de chamadas de cada um deles, tomar conhecimento sobre os \textit{commits} dos métodos que possivelmente foram os responsáveis pelo desvio. A partir de então, ações podem ser tomadas pela equipe de desenvolvimento para sanar possíveis problemas no desempenho das aplicações. O estudo foi guiado pelas seguintes questões de pesquisa:

\textbf{QP1. A ferramenta proposta é capaz de exibir representações visuais que fornecem informações sobre os cenários de determinado \textit{release} de um sistema que tiveram o maior desvio de desempenho dentre os analisados?} Após as análises realizadas nos \textit{releases} dos sistemas, a expectativa é que a visualização da sumarização de cenários seja capaz de exibir metáforas visuais que permitam aos usuários identificar informações sobre os cenários. É esperado que a complexidade na identificação dessas informações seja baixa através da metáfora visual e interações oferecidas para ajudar os usuários nessa identificação.

\textbf{QP2. A ferramenta proposta é capaz de exibir visualmente, para determinado cenário analisado, os métodos identificados com desvios de desempenho, além dos \textit{commits} que possivelmente causaram esses desvios?} A expectativa é que a visualização do grafo de chamada seja capaz de exibir elementos visuais que permitam aos usuários encontrar as informações sobre os métodos com desvios de desempenho, além das suas possíveis causas. Espera-se, também, que a complexidade de identificação dessas informações seja baixa através das metáforas visuais oferecidas e das interações implementadas. É importante que os usuários tomem conhecimento sobre as modificações no código-fonte que geraram algum impacto no desempenho dos sistemas.

\textbf{QP3. Os usuários dos sistemas analisados veem benefícios ou vantagens de se usar a ferramenta proposta em seus processos de desenvolvimento?} Espera-se, com essa questão de pesquisa, verificar se os usuários dos sistemas analisados utilizariam a ferramenta em seus processos de desenvolvimento. Além disso, outro aspecto interessante é saber em qual momento desse processo os desenvolvedores vislumbram que a ferramenta pode ser usada.

\subsection{Sistemas e Procedimentos} \label{subsec:avaliacao-sistemas-procedimentos}

Neste estudo foram usados dois sistemas \textit{open-source} de diferentes domínios escolhidos através dos seguintes critérios: (i) ser desenvolvido na linguagem de programação Java; (ii) ter no mínimo dez \textit{releases}; (iii) possuir casos de testes automatizados utilizando a biblioteca JUnit; (iv) estar listado em uma das categorias do site \href{http://java-source.com}{http://java-source.com} (site de projetos \textit{open-source} em Java). Os sistemas foram escolhidos de modo que não houvesse repetição de categorias; (v) possuir repositório no GitHub.

A partir desses critérios, foram escolhidos os sistemas Jetty \cite{Jetty2016} e o VRaptor \cite{VRaptor2017}. O Jetty é um servidor web e um servlet contêiner Java capaz de fornecer conteúdo estático e dinâmico a partir de instanciações \textit{standalone} ou embutidas. As dez últimas \textit{releases} estáveis do Jetty foram escolhidas no momento do início do estudo. Dessa forma, as versões escolhidas foram 9.3.10, 9.3.11, 9.3.12, 9.3.13, 9.3.14, 9.3.15, 9.3.16, 9.4.0, 9.4.1 e 9.4.2.

O VRaptor é um \textit{framework} MVC para desenvolvimento web em Java. Visa trazer alta produtividade para um desenvolvimento com CDI. As dez últimas \textit{releases} estáveis do VRaptor foram escolhidas no momento do início do estudo. Assim, as versões escolhidas foram 4.0.0.Final, 4.1.0.Final, 4.1.1, 4.1.2, 4.1.3, 4.1.4, 4.2.0.RC1, 4.2.0.RC2, 4.2.0.RC3 e 4.2.0.RC4.

{\color{red}ESTOU AQUI...}

Uma vez escolhidos os sistemas e as versões de cada um, os procedimentos adotados são os seguintes:
\begin{enumerate}[(i)]
	\item Os códigos-fonte dos sistemas a serem analisados são obtidos do repositório, para cada uma das versões a serem analisadas. Todos os testes de sistema são considerados como pontos de entrada de cenários. Para o cálculo do desvio de desempenho, o \textit{PerfMiner} não considera quaisquer desvios a partir de classes de teste, somente de classes do código-fonte da aplicação;
	\item Os sistemas são configurados para dar suporte ao \textit{AspectJ} e são incluídas as bibliotecas do \textit{PerfMiner}, no entanto, sem qualquer alteração no código-fonte. Dessa forma, a análise dinâmica será executada no mesmo computador, sob as mesmas condições, para todos os sistemas. Para minimizar o impacto no resultado da análise, serviços essenciais do sistema operacional serão desativados;
	\item Após a execução da análise dinâmica, o \textit{PerfMiner} executa testes estatísticos para determinar a relevância dos resultados obtidos para, então, criar e armazenar os artefatos de saída;
	\item De posse dos artefatos de saída, é realizado um processamento em lote pertencente à extensão implementada. O processamento se assemelha ao descrito na figura \ref{fig:passos-2-3-grafo-chamadas}, no entanto, sem a atividade de enviar a resposta com o arquivo JSON. Após isso, todos os dados que dão suporte às três visualizações propostas estarão salvos no banco de dados QAV (figura \ref{fig:funcionamento-geral-visualizacoes});
	\item Por fim, as visualizações serão acessadas a partir de um navegador web e serão verificados os resultados gerados.
\end{enumerate}

Nesse primeiro momento do estudo, a avaliação dos resultados é feita sem a participação dos desenvolvedores e arquitetos dos sistemas analisados. Em um segundo momento, as visualizações serão disponibilizadas em um serviço web na nuvem, como o Heroku\footnote{https://www.heroku.com/} ou OpenShift\footnote{https://www.openshift.com/}, para que os desenvolvedores e arquitetos possam acessar as visualizações através do navegador. A partir do uso da ferramenta por esses usuários, será possível coletar o \textit{feedback} através de questionários.

\section{Resultados} \label{sec:avaliacao-resultados}
\section{Considerações} \label{sec:avaliacao-consideracoes}