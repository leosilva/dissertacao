% Introdução
\chapter{Introdução} \label{ch:introducao}

A crescente utilização dos softwares por usuários com diferentes características e executando em dispositivos distintos, faz com que seja alta a demanda por novas funcionalidades e tecnologias ou para reparação de erros. Caserta e Zendra \cite{Caserta2011} mencionam que um software se torna rapidamente complexo quando o seu tamanho aumenta, trazendo dificuldades para o seu entendimento, manutenção e evolução. Nesse sentido, a manutenção e evolução de sistemas de software tem se tornado uma tarefa difícil e crítica com o passar dos anos.

O processo de manutenção é conhecido como o mais caro e o que mais consome tempo dentro do ciclo de vida de um software \cite{Abreu1995}. A maior parte do tempo gasto nesse processo é dedicada a compreender o sistema, especialmente se os desenvolvedores não estiverem envolvidos desde o início do desenvolvimento. Os desenvolvedores podem gastar mais de 60\% do esforço de manutenção entendendo o software \cite{Corbi1989}.

Nesse contexto, a evolução de um software tem sido um dos tópicos mais estudados da engenharia de software. Grandes quantidades de dados originados de locais diferentes estão envolvidos na evolução de um software, como repositórios de gerenciamento de código-fonte \abrv[SCM -- \textit{Source Code Management}]{(do inglês \textit{Source Code Management} -- SCM),} sistemas de rastreamento de erros \abrv[BTS -- \textit{Bug Tracking System}]{(do inglês \textit{Bug Tracking System} -- BTS)}e listas de e-mail. Um dos principais objetivos da área de evolução de software é construir teorias e modelos que permitam compreender o passado e o presente, dessa forma, apoiando as tarefas do processo de manutenção de software \cite{Novais2013}.

A área de Visualização de Software provê representações visuais com o intuito de tornar o software mais compreensível. Essas representações se fazem necessárias para os analistas, arquitetos e desenvolvedores examinarem os softwares devido a sua natureza complexa, abstrata e difícil de ser observada \cite{Petre2006}. Essa área pode focar em vários aspectos de sistemas de software, como padrões de projeto, arquitetura, processo de desenvolvimento, histórico de código-fonte, esquemas de banco de dados, interações de rede, processamento paralelo, execução de processos, dentre outros \cite{Ghanam2008}. Entretanto, um dos componentes essenciais é a visualização da arquitetura de um software e seus atributos de qualidade.

%Este capítulo introduz este trabalho. A seção \ref{sec:apresentacao-do-problema} exibe o contexto do trabalho e apresenta o problema a ser tratado. A seção \ref{sec:limitacao-abordagens-atuais} apresenta as principais limitações das abordagens atuais. A seção \ref{sec:abordagem-proposta} apresenta a abordagem proposta, ao passo que a seção \ref{sec:objetivos-gerais-especificos} discute os objetivos gerais e específicos. Por fim, a organização do trabalho é mostrada na seção \ref{sec:organizacao-trabalho}.

\section{Apresentação do Problema} \label{sec:apresentacao-do-problema}

Ghanam e Carpendale \cite{Ghanam2008} afirmam que não apenas os arquitetos estão interessados em visualizar arquiteturas de software, mas também os desenvolvedores, testadores, gerentes de projetos e até mesmo os clientes. Um dos principais desafios dessas visualizações é descobrir representações visuais eficientes e eficazes para exibir a arquitetura de um software juntamente com as métricas de código envolvidas. Essa visualização se torna especialmente difícil quando é adicionada a variável tempo, passando a ser necessária a representação de sua evolução. Desse modo, a quantidade de dados envolvidos aumenta uma vez que todas as versões do software passam a ser consideradas \cite{Caserta2011}\cite{Khan2012}.

Dentre os atributos de qualidade de uma arquitetura de software, o desempenho pode ser considerado um dos mais importantes para sistemas de software. Falhas de desempenho podem resultar em relações com o cliente prejudicadas, perda de produtividade para os usuários, aumento dos custos de desenvolvimento, etc \cite{Williams1998}. A maioria das falhas de desempenho se dá devido à falta de acompanhamento de problemas dessa natureza no início do processo de desenvolvimento \cite{Williams1998}.

A evolução de software se refere às mudanças dinâmicas de características e comportamento de um software ao longo do tempo \cite{Pablo2016}. Nesse processo, as modificações progressivas podem ter consequências negativas, diminuindo a qualidade do software e aumentando a sua complexidade \cite{BeladyLaszloA.andLehman1976}\cite{LehmanMeirMandRamilJuanFandWernickPaulDandPerryDewayneEandTurski}. Essa deterioração pode também afetar o desempenho dos sistemas ao longo do tempo \cite{Molyneaux2009}. \citeauthor{SandovalAlcocer2013} reforçam que mudanças no código-fonte podem causar comportamentos inesperados em tempo de execução, como, por exemplo, o desempenho de partes da aplicação podem ser degradados em uma nova versão em comparação com a versão anterior. Nesse contexto, sem o devido acompanhamento, os atributos de qualidade, como o desempenho, inicialmente definidos a partir de decisões arquiteturais e de \textit{design} tomadas durante o processo de desenvolvimento podem deixar de ser bem atendidos.

%Nesse contexto, a inexistência de atividades para que os desenvolvedores possam entender a evolução arquitetural pode levar a sua degradação \cite{DAmbros2009}, fazendo com que os atributos de qualidade inicialmente definidos, a partir de decisões arquiteturais e de \textit{design} tomadas durante o processo de desenvolvimento, deixem de ser atendidos.

%Durante o processo de manutenção, mudanças no código-fonte podem causar comportamentos inesperados em tempo de execução, como, por exemplo, o desempenho de partes da aplicação podem ser degradados em uma nova versão em comparação com a versão anterior \cite{SandovalAlcocer2013}. Nesse contexto, a inexistência de atividades para que os desenvolvedores possam entender a evolução arquitetural pode levar a sua degradação \cite{DAmbros2009}, fazendo com que os atributos de qualidade inicialmente definidos, a partir de decisões arquiteturais e de \textit{design} tomadas durante o processo de desenvolvimento, deixem de ser atendidos.

%Diante disso, é importante que alguma abordagem de visualização da evolução arquitetural de um software, em especial quando se trata de um importante atributo de qualidade como o desempenho, auxilie os arquitetos, desenvolvedores e analistas no processo de desenvolvimento, diminuindo a probabilidade de ocorrer a degradação da arquitetura durante o processo de manutenção.

%Diante disso, é importante que alguma abordagem visual para analisar a evolução do atributo de qualidade de desempenho auxilie os desenvolvedores e arquitetos no processo de desenvolvimento, de modo a diminuir a probabilidade de ocorrer a degradação desse atributo de qualidade durante o processo de manutenção.

Diante disso, é importante a definição de uma abordagem visual para facilitar a análise da evolução do atributo de qualidade de desempenho para os desenvolvedores e arquitetos dos sistemas, de modo a ajudá-los a gerenciar essa evolução e, consequentemente, diminuir a probabilidade de ocorrer variações não planejadas desse atributo de qualidade durante o processo de manutenção ou desenvolvimento de uma nova funcionalidade.

\section{Limitações das Abordagens Atuais} \label{sec:limitacao-abordagens-atuais}

De acordo com a literatura, existem várias ferramentas que tratam da visualização da evolução arquitetural de softwares, inclusive relacionadas ao monitoramento de desempenho, cada uma com suas particularidades. Caserta e Zendra \cite{Caserta2011} apontam que essas ferramentas apresentam a evolução arquitetural a partir de: (i) como a arquitetura global é alterada a cada versão, incluindo mudanças no código fonte; (ii) como os relacionamentos entre os componentes evoluem; e (iii) como as métricas evoluem a cada \textit{release}. Este trabalho considera tais aspectos focalizando especificamente o atributo de qualidade de desempenho, em termos de tempo de execução.

As ferramentas de \textit{profiling} realizam análise desse atributo de qualidade no software, porém com características diferentes. O \textit{JProfiler} \cite{JProfiler} e o \textit{YourKit Java Profiler} \cite{Profiler2016} são ferramentas comerciais que realizam essa análise para a linguagem Java. Elas exibem um grafo de chamadas dos métodos com seus respectivos tempos de execução. O usuário pode, à medida que deseja, tirar fotografias instantâneas da execução do software, os chamados \textit{snapshots}. A partir dessas fotografias é possível realizar a comparação, porém é uma atividade manual, necessitando da ação do usuário para escolher quais \textit{snapshots} serão comparados. A \textit{VisualVM} \cite{Vis}, distribuída gratuitamente com o \textit{Java Development Kit} \abrv[JDK -- \textit{Java Development Kit}]{(JDK),} não oferece comparação entre \textit{snapshots}.

%As ferramentas de \textit{profiling} realizam análise desse atributo de qualidade no software, porém com características diferentes. A \textit{VisualVM} \cite{Vis}, distribuída gratuitamente com o \textit{Java Development Kit} \abrv[JDK -- \textit{Java Development Kit}]{(JDK),}exibe o tempo de execução de cada método em tempo real e o usuário pode, à medida que deseja, tirar fotografias instantâneas da execução do software, os chamados \textit{snapshots}. Essa ferramenta, no entanto, não oferece a comparação do tempo de execução do mesmo método em versões anteriores do software, tornando difícil a visualização da evolução do atributo de qualidade de desempenho, uma vez que teria que ser feita manualmente para cada método desejado.

%Já o \textit{JProfiler} \cite{JProfiler}, ferramenta paga, pode exibir o grafo de chamadas dos métodos em tempo real, com seus respectivos tempos de execução. Assim como o \textit{VisualVM}, a ferramenta oferece a possibilidade de guardar snapshots de determinados momentos da execução. Contudo, os \textit{snapshots} não são automáticos, o usuário precisa, deliberadamente, informar à ferramenta quando ele deve ser acionado. Uma maneira de contornar esse problema é fazendo o uso de \textit{triggers}, onde o usuário pode configurar a ferramenta para responder a determinados eventos da \textit{Java Virtual Machine} \abrv[JVM -- \textit{Java Virtual Machine}]{(JVM)} e, assim, executar algumas ações. Apesar da funcionalidade ser interessante e poderosa, dependendo do que o usuário deseja, a configuração das \textit{triggers} pode se tornar maçante. A ferramenta oferece a comparação entre os \textit{snapshots}, porém, não é automática e necessita da ação do usuário para escolher quais \textit{snapshots} serão comparados. A ferramenta \textit{YourKit Java Profiler} \cite{Profiler2016} possui funcionalidades semelhantes às comentadas para o \textit{JProfiler}. Entretanto, é uma ferramenta paga e a comparação entre os \textit{snapshots} também não é automática.

Com exceção do \textit{VisualVM}, as ferramentas de \textit{profiling} mencionadas possuem uma característica em comum: a forma com que apresentam a evolução do atributo de qualidade de desempenho não é automática e tampouco é direcionada ao(s) método(s) desejado(s) pelo usuário. Apesar de essas ferramentas serem úteis para acompanhar o desempenho geral, comparar a diferença dos tempos dos métodos é muitas vezes insuficiente para compreender as razões para a variação de desempenho \cite{SandovalAlcocer2013}.

%Com exceção do \textit{VisualVM}, as ferramentas de \textit{profiling} mencionadas possuem uma característica em comum: a forma com que apresentam a evolução do atributo de qualidade de desempenho não é automática e tampouco é direcionada ao(s) método(s) desejado(s) pelo usuário. É necessário selecionar manualmente os \textit{snapshots} que se deseja comparar e as ferramentas exibem duas formas de visualização da evolução do desempenho, em geral: \textit{call tree} e \textit{hot spot}. Em ambas, são apresentados todos os métodos monitorados, cabendo ao usuário procurar o método desejado para, então, verificar qual a sua evolução. Apesar de essas ferramentas serem úteis para acompanhar o desempenho geral, comparar a diferença dos tempos dos métodos é muitas vezes insuficiente para compreender as razões para a variação de desempenho \cite{SandovalAlcocer2013}.

%Corroborando com o exposto para o \textit{JProfiler} e o \textit{YourKit Java Profiler}, \citeauthor{SandovalAlcocer2013} mencionam que essas duas ferramentas, apesar de serem úteis para acompanhar o desempenho geral, comparar a diferença dos tempos dos métodos é muitas vezes insuficiente para compreender as razões para a variação de desempenho. Os autores listam algumas limitações dessas duas ferramentas, tais como: as variações de desempenho têm que ser manualmente rastreadas e as visualizações utilizadas são ineficientes. Essas ferramentas também não oferecem maiores detalhes sobre os desvios, tais como: código-fonte dos métodos, possíveis \textit{commits} que introduziram o desvio e quais tarefas estariam relacionadas com a degradação ou melhoria encontrada. Novamente, caso o usuário queira identificar tais características para determinado método com desvio terá que pesquisar manualmente diretamente na fonte dos dados: repositório de código-fonte e sistema de gerenciamento de tarefas.

Ferramentas de gerenciamento de desempenho de aplicações também podem ser utilizadas para a identificação de regressões de desempenho, como conclui \citeauthor{Ahmed2016}. Os autores definem regressão de desempenho quando as atualizações em um software provocam uma degradação no seu desempenho. As ferramentas utilizadas no estudo foram \textit{New Relic} \cite{Relic2016}, \textit{AppDynamics} \cite{Appdynamics}, \textit{Dynatrace} \cite{Dynatrace2016} e \textit{Pinpoint} \cite{Pinpoint2016}. Entretanto, a identificação da causa da regressão foi complicada, sendo necessário bastante trabalho manual \cite{Ahmed2016}. De maneira semelhante às ferramentas de \textit{profiling}, o processo não é automático e não existem visualizações adequadas que esclareçam a regressão de desempenho.

Há abordagens de variação de desempenho que indicam as possíveis causas de desvios de desempenho de sistemas. O trabalho de \citeauthor{SandovalAlcocer2013} apresenta uma visualização de grafo de chamadas para entender a causa de degradações de desempenho entre duas versões de um sistema. No entanto, faltam informações sobre a hierarquia de chamadas que melhor caracterizem os nós apresentados, uma avaliação mais consistente e um maior potencial de aplicabilidade da visualização. Outros trabalhos apresentam visualizações para identificação de desvios de desempenho \cite{Bergel}\cite{Mostafa2009}, porém não indicam métodos adicionados ou removidos na hierarquia de chamadas nem apresentam as possíveis causas desses desvios. \citeauthor{Bezemer2015} utilizam gráficos de chama diferenciais para comparação do desempenho entre duas versões de um software. Nesse trabalho, poucas métricas são utilizadas com relação aos métodos executados, além de também não representarem os métodos adicionados ou removidos durante a execução e não mostrarem as possíveis causas para os desvios de desempenho.

%\citeauthor{Ahmed2016} realizaram um estudo para verificar se as ferramentas de gerenciamento de desempenho de aplicações\abrv[APM -- \textit{Application Performance Management}]{}(APM, do inglês \textit{Application Performance Management}) são eficazes na identificação de regressões de desempenho. Os autores definem regressão de desempenho quando as atualizações em um software provocam uma degradação no seu desempenho \cite{Ahmed2016}. As ferramentas utilizadas no estudo foram \textit{New Relic} \cite{Relic2016}, \textit{AppDynamics} \cite{Appdynamics}, \textit{Dynatrace} \cite{Dynatrace2016} e \textit{Pinpoint} \cite{Pinpoint2016}. Como resultado, eles mostram que a maioria das regressões inseridas no código-fonte foram detectadas pelas ferramentas. Contudo, o processo de identificação da causa da regressão, ou seja, o método exato cujo código foi inserido, foi mais complicado, sendo necessário bastante trabalho manual: os autores inspecionavam as transações (requisições) marcadas como lentas e, manualmente, comparavam os respectivos \textit{stacktraces} para verificar se a ferramenta indicava corretamente a regressão de desempenho. O processo, mais uma vez, não é automático e não existem visualizações adequadas que esclareçam a regressão de desempenho.

A partir das abordagens existentes verifica-se que há espaço para melhorias nas visualizações propostas por essas abordagens, bem como há necessidade de novas visualizações da evolução do software com foco no atributo de qualidade de desempenho. Além disso, também pode ser aprimorada a automação completa (ou parcial) do processo de análise, uma vez que ainda é necessária considerável intervenção manual do usuário nas ferramentas existentes para visualizar a evolução de diferentes versões do sistema. Além da necessidade de novas visualizações, outros requisitos de uma abordagem para análise de desvios de desempenho na evolução de sistemas são mencionados por Pinto \cite{Pinto2015}:
\begin{itemize}
	\item Deveria automatizar o processo ao máximo. Técnicas manuais consomem tempo e são custosas, além de não se mostrarem adequadas se o processo de avaliação requerer a análise de sucessivas evoluções;
	\item Os softwares podem ser muitos grandes para uma análise completa. Dessa forma, a ferramenta deveria focar em partes selecionadas do sistema;
	\item A ferramenta deveria ser capaz de medir o desempenho de determinados cenários e seus métodos a fim de identificar onde ocorreu o desvio;
	\item Deveria prover suporte para análise do código-fonte com o intuito de oferecer feedback detalhado sobre o código relacionado com o desvio detectado;
	\item Deveria ser capaz de acessar dados dos repositórios do software, como ferramentas de controle de versão e sistemas de gerenciamento de tarefas, com a finalidade de exibir as mudanças relacionadas ao código com desvio de desempenho.
\end{itemize}

\section{Abordagem Proposta} \label{sec:abordagem-proposta}

Este trabalho apresenta uma ferramenta, chamada \textit{\toolName}, cujo objetivo é aplicar técnicas de visualização de software para ajudar desenvolvedores e arquitetos a analisar a evolução do atributo de qualidade de desempenho, em termos de tempo de execução, ao longo das versões de um software. A ferramenta propõe duas visualizações com escopos e granularidades diferentes e proporciona ao usuário mecanismos de interação para explorá-las.

O tempo de execução, neste trabalho, é o tempo que um dado método ou cenário\footnote{Um cenário é definido como uma ação de alto nível que representa a maneira como os \textit{stakeholders} interagem com o sistema. No contexto deste trabalho, cada caso de teste automatizado do sistema será considerado um cenário.} do sistema demora para executar. Pode também ser tratado como tempo de resposta. Para medir o desempenho, além do tempo de execução, outras propriedades podem ser usadas, como: consumo de memória, entrada e saída de disco, uso do processador e tráfego de rede \cite{Malik2013}. A medição desse atributo em termos de tempo de execução foi escolhida por se tratar de uma propriedade geral e comum para a capacidade de resposta de um sistema.

%Este trabalho visa aplicar técnicas de visualização de software para acompanhar a evolução arquitetural do atributo de qualidade de desempenho. Pretende-se que as visualizações sejam capazes de exibir os cenários e métodos relacionados que tiveram desvios de desempenho durante a avaliação de diferentes versões do sistema, de forma direta e fácil.

A ferramenta foi implementada como extensão a outra já existente, chamada \textit{\perfMinerName} \cite{Pinto2015}. Essa ferramenta busca apontar quais cenários degradaram ou otimizaram o atributo de qualidade de desempenho. A escolha desse atributo de qualidade se deu pelo fato de ser uma propriedade crítica para a maioria dos sistemas de software atuais.

A extensão proposta visa oferecer um melhor entendimento da evolução desse atributo de qualidade na arquitetura de um software por parte dos arquitetos e desenvolvedores, beneficiando-os ao: (i) fornecer uma visão geral dos cenários com desvios de desempenho entre uma determinada versão do software e a anterior; (ii) possibilitar a identificação dos cenários que possuem elevado tempo de execução; (iii) saber qual desses cenários teve o maior desvio de desempenho; (iv) acompanhar a evolução de cada cenário ao longo das versões analisadas; (v) saber, para cada cenário, os métodos que foram detectados com algum tipo de desvio, além de ter conhecimento sobre os métodos que foram adicionados e removidos; e (vi) fornecer uma listagem de \textit{commits} que possivelmente foram as causas dos desvios de desempenho identificados. Com base nisso, a equipe de desenvolvimento pode tomar ações para sanar possíveis problemas no desempenho das aplicações além de acompanhar a evolução, planejada ou não, desse atributo de qualidade.

% perceber quais cenários de casos de uso degradaram ou melhoraram o seu desempenho; (ii) identificar qual trecho de código-fonte foi o responsável por uma dada degradação; e (iii) identificar questões de desenvolvimento associadas, como quando e qual desenvolvedor realizou mudanças relacionadas à degradação, e qual tarefa está relacionada a alteração no sistema de gerenciamento de tarefas.

\section{Objetivos Gerais e Específicos} \label{sec:objetivos-gerais-especificos}

O objetivo principal deste trabalho é implementar uma ferramenta com o intuito de prover um conjunto de visualizações de modo a facilitar o entendimento da evolução do atributo de qualidade de desempenho. A implementação dessa ferramenta foi feita estendendo outra ferramenta já existente desenvolvida pelo grupo de pesquisa \abrv[CASE -- \textit{Collaborative \& Automated Software Engineering Research Group}]{\textit{Collaborative \& Automated Software Engineering Research Group} (CASE),}do Departamento de Informática e Matemática Aplicada \abrv[DIMAp -- Departamento de Informática e Matemática Aplicada]{(DIMAp)}da \abrv[UFRN -- Universidade Federal do Rio Grande do Norte]{UFRN.}

A ferramenta estendida, chamada de \textit{\perfMinerName} \cite{Pinto2015}, pode ser definida como uma abordagem automatizada baseada em cenários para identificar desvios de desempenho, em termos de tempo de execução. A ferramenta indica, também, quais trechos de código-fonte podem ter causado a variação de desempenho baseado na mineração de \textit{commits} e tarefas (\textit{issues}) de desenvolvimento. Técnicas de análise dinâmica e mineração de repositórios de software são usadas pela ferramenta para atingir os seus objetivos. Nesse contexto, os objetivos específicos deste trabalho são:
\begin{itemize}
	\item Investigar as principais abordagens de visualização da evolução do atributo de qualidade de desempenho, a fim de conhecer quais técnicas são utilizadas atualmente, e identificar lacunas que servem como motivação para o desenvolvimento do trabalho;
	\item Projetar e implementar a ferramenta \textit{{\toolName}}. A ferramenta é desenvolvida na linguagem de programação Groovy, de modo que a análise de desempenho é suportada para sistemas desenvolvidos na linguagem Java, além do Groovy;
	\item Conduzir estudos de avaliação da ferramenta para avaliar a utilidade e eficácia das visualizações para encontrar informações sobre os desvios de desempenho e a aplicabilidade da ferramenta como parte integrante dos processos de desenvolvimento desses sistemas.
\end{itemize}

\section{Organização do trabalho} \label{sec:organizacao-trabalho}

Este trabalho está organizado como segue: o capítulo \ref{ch:fundamentacao-teorica} apresenta a fundamentação teórica para o entendimento do trabalho, tais como arquitetura de software, visualização de software e ferramentas de análise de desempenho. O capítulo \ref{ch:pqae} apresenta a solução proposta, mostrando as visualizações definidas e implementadas para melhorar o entendimento das análises, além de explicar o funcionamento do \textit{\perfMinerName} integrado com o conjunto de visualizações proposto. O capítulo \ref{ch:avaliacao} discute o estudo aplicado para a avaliação da ferramenta e suas visualizações. O capítulo \ref{ch:trabalhos-relacionados} exibe trabalhos relacionados, mencionando as suas limitações e comparando-os com este trabalho. Finalmente, o capítulo \ref{ch:conclusao} apresenta as conclusões do trabalho, discute as limitações e propõe trabalhos futuros.